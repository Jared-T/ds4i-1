[
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "The Bag-of-Words (BoW) representation is a simplistic yet effective method for text data representation. It hinges on representing text by its constituent words, disregarding their order. Here, each word operates as a feature, with the text being represented by a vector that denotes the frequency of each word.\nFormally, given a vocabulary \\(V\\) comprising \\(N\\) unique words, each document $d $can be depicted as a vector $d $in $^N ), where the i-th element $v{d,i} $denotes the frequency of the i-th word in the document:\n\\[\n\\mathbf{v}_d = [v_{d,1}, v_{d,2}, \\ldots, v_{d,N}]\n\\]\nThe dataset was transformed into a BoW representation with each row corresponding to a sentence, and each column reflecting the frequency of a word in that sentence. The CountVectorizer class from the sklearn.feature_extraction.text module was employed for this task, with English stop words being excluded to filter out prevalent words that lack significant meaning, such as “and”, “the”, and “is”.\n\n\n\nContrastingly, the TF-IDF representation scales the frequency of words based on their occurrence across all documents, ensuring that words appearing too frequently across documents (potentially bearing lesser discriminative importance) are assigned lower weights.\nThe term frequency (TF) of a word in a document is the raw count of that word in the document. The inverse document frequency (IDF) of a word is defined as:\n\\[\n\\text{IDF}(w) = \\log \\left( \\frac{N}{1 + \\text{count}(w)} \\right)\n\\]\nwhere $N $signifies the total number of documents and $(w) $represents the number of documents containing the word $w ). The TF-IDF value for a word in a document is then the product of its TF and IDF values.\nThe TfidfVectorizer class from the sklearn.feature_extraction.text module was employed to transform our dataset into this representation.\n\n\n\nFor processing by deep learning models like neural networks, textual data was tokenized and converted into sequences of numbers. The Tokenizer class from the keras.preprocessing.text module was utilized for this purpose. Subsequently, sentences were padded with zeros using pad_sequences from the keras.preprocessing.sequence module to ensure uniform length."
  },
  {
    "objectID": "methods.html#text-representation-techniques",
    "href": "methods.html#text-representation-techniques",
    "title": "Methods",
    "section": "",
    "text": "The Bag-of-Words (BoW) representation is a simplistic yet effective method for text data representation. It hinges on representing text by its constituent words, disregarding their order. Here, each word operates as a feature, with the text being represented by a vector that denotes the frequency of each word.\nFormally, given a vocabulary \\(V\\) comprising \\(N\\) unique words, each document $d $can be depicted as a vector $d $in $^N ), where the i-th element $v{d,i} $denotes the frequency of the i-th word in the document:\n\\[\n\\mathbf{v}_d = [v_{d,1}, v_{d,2}, \\ldots, v_{d,N}]\n\\]\nThe dataset was transformed into a BoW representation with each row corresponding to a sentence, and each column reflecting the frequency of a word in that sentence. The CountVectorizer class from the sklearn.feature_extraction.text module was employed for this task, with English stop words being excluded to filter out prevalent words that lack significant meaning, such as “and”, “the”, and “is”.\n\n\n\nContrastingly, the TF-IDF representation scales the frequency of words based on their occurrence across all documents, ensuring that words appearing too frequently across documents (potentially bearing lesser discriminative importance) are assigned lower weights.\nThe term frequency (TF) of a word in a document is the raw count of that word in the document. The inverse document frequency (IDF) of a word is defined as:\n\\[\n\\text{IDF}(w) = \\log \\left( \\frac{N}{1 + \\text{count}(w)} \\right)\n\\]\nwhere $N $signifies the total number of documents and $(w) $represents the number of documents containing the word $w ). The TF-IDF value for a word in a document is then the product of its TF and IDF values.\nThe TfidfVectorizer class from the sklearn.feature_extraction.text module was employed to transform our dataset into this representation.\n\n\n\nFor processing by deep learning models like neural networks, textual data was tokenized and converted into sequences of numbers. The Tokenizer class from the keras.preprocessing.text module was utilized for this purpose. Subsequently, sentences were padded with zeros using pad_sequences from the keras.preprocessing.sequence module to ensure uniform length."
  },
  {
    "objectID": "methods.html#model-architectures-and-training",
    "href": "methods.html#model-architectures-and-training",
    "title": "Methods",
    "section": "2. Model Architectures and Training",
    "text": "2. Model Architectures and Training\n\na. Feed-Forward Neural Network\nFeed-forward neural networks (FFNNs) are a subset of artificial neural networks characterized by acyclic connections between nodes. They encompass multiple layers: an input layer, several hidden layers, and an output layer.\nThe architecture of the neural network employed in this study is delineated as follows:\n\nInput Layer: This layer harbors neurons equal to the number of features in the dataset (word counts for BoW and TF-IDF, sequence length for text embeddings). The Rectified Linear Unit (ReLU) activation function was utilized owing to its efficiency and capability to mitigate the vanishing gradient issue:\n\n\\[\nf(x) = \\max(0, x)\n\\]\n\nHidden Layers: Several hidden layers were introduced, each utilizing He initialization, which is proficient for layers with ReLU activation. A dropout layer succeeded each hidden layer to curb overfitting by randomly nullifying a fraction of input units during each training update.\nOutput Layer: This layer contains neurons equal to the number of classes (presidents, in our scenario). The softmax function was employed as the activation function, generating a probability distribution over the classes:\n\n\\[\n\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n\\]\nfor $i = 1, , K $and $ $is the input vector to the softmax function.\nTraining was conducted using the Adam optimization algorithm with a learning rate of 0.001. Adam is adept at training deep neural networks via computing adaptive learning rates for each parameter, leveraging moving averages of the parameter gradients and squared gradients.\nThe EarlyStopping and ReduceLROnPlateau callbacks were also enlisted. The former halts the training process if validation loss ceases to improve for a stipulated number of epochs, while the latter diminishes the learning rate if the validation loss reaches a plateau.\n\n\nb. Support Vector Machine (SVM)\nThe Support Vector Machine (SVM) is a supervised learning algorithm suitable for both classification and regression tasks. It operates by identifying the optimal hyperplane that segregates a dataset into distinct classes. Provided a set of training examples, each labeled as belonging to one of two categories, the SVM training algorithm constructs a model that categorizes new examples into one of the two categories.\nMathematically, given labeled training data $(x_1, y_1), , (x_N, y_N) $where $ x_i $belongs to $ ^D $and $ y_i $is either 1 or -1 (indicating the class the input $ x_i $belongs to), SVM seeks the hyperplane defined by $ w $and $ b $that optimally separates the data points of the two classes:\n\\[\ny_i(w \\cdot x_i + b) \\geq 1\n\\]\nThe objective of SVM is to maximize the margin, which is the distance between the hyperplane and the nearest point from either class. The decision function is then given by:\n\\[\nf(x) = \\text{sign}(w \\cdot x + b)\n\\]\n\n\nc. Naive Bayes Classifier\nNaive Bayes is a probabilistic classifier predicated on Bayes’ theorem with strong (naive) independence assumptions among features. Given a set of features $X = x_1, , x_n $and a class variable $C ), Bayes’ theorem states:\n\\[\nP(C|X) = \\frac{P(X|C) \\times P(C)}{P(X)}\n\\]\nThe Naive Bayes classifier posits that the effect of a particular feature in a class is independent of other features. This simplification expedites computation, hence the term ‘naive’.\nIn our problem, the Naive Bayes classifier estimates the probability of a sentence belonging to each president’s class based on the features (word frequencies for BoW or TF-IDF values). The sentence is then classified to the class (president) with the highest probability."
  },
  {
    "objectID": "methods.html#model-evaluation",
    "href": "methods.html#model-evaluation",
    "title": "Methods",
    "section": "3. Model Evaluation",
    "text": "3. Model Evaluation\nEvaluating the performance of machine learning models is paramount as it unveils the efficacy of the model and areas of potential improvement. Our evaluation paradigm leverages standard metrics including accuracy, precision, recall, and F1 score to quantify various facets of the model’s predictions in a multi-class classification setting such as ours, where predictions could be true or false for multiple classes (presidents, in this case).\n\na. Accuracy\nAccuracy furn\nishes a broad overview of the model’s performance and is calculated as the ratio of correct predictions to the total predictions:\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\nNonetheless, in imbalanced datasets, accuracy could be misleading.\n\n\nb. Precision\nPrecision scrutinizes the model’s positive predictions. Specifically, it computes the frequency at which the model correctly predicted a specific president out of all predictions for that president:\n\\[\n\\text{Precision (for a given president)} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\nWhere: - True Positives (TP): The number of sentences correctly identified as belonging to that president. - False Positives (FP): The number of sentences erroneously identified as belonging to that president, while they belong to a different one.\nPrecision is particularly crucial in scenarios where the cost of a false positive is high.\n\n\nc. Recall (or Sensitivity)\nRecall evaluates how effectively the model identifies sentences from a specific president. It calculates the proportion of actual sentences from a president that the model correctly identified:\n\\[\n\\text{Recall (for a given president)} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\nWhere: - False Negatives (FN): The number of sentences that genuinely belong to a president but were misclassified as belonging to another.\nRecall is vital in contexts where missing a true instance is significant.\n\n\nd. F1 Score\nThe F1 score is the harmonic mean of precision and recall, providing a balance between them. It achieves its best value at 1 (perfect precision and recall) and its worst at 0:\n\\[\n\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nThe F1 score is particularly useful when there is an uneven data distribution among classes.\nThese metrics were computed for each president in our dataset and then averaged (weighted by the number of true instances for each president) to derive a single value representing the overall model’s performance. This approach ensures that the model’s aptitude to predict less frequent classes (presidents with fewer sentences) is considered, rendering the evaluation more robust and representative of the model’s true capabilities in a multi-class setting.\nMoreover, the models were also assessed on separate training and test datasets. The training dataset is the learning corpus for the model, while the test dataset presents a fresh, unseen set of data points to gauge the model’s generalization to new data. This separation is pivotal to ensure that the model doesn’t merely memorize the training data (overfitting), but discerns the underlying patterns determining which president uttered a given sentence.\nLastly, model validation during training utilized early stopping and learning rate reduction based on validation loss, ensuring the model doesn’t over-train and begins to perform worse on unseen data. Callbacks in the neural network training process monitored the validation loss and amended the training process accordingly."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "In the exploration of various models on a training set employing a bag of words representation, distinct performance disparities were observed. Firstly, utilising a feed-forward neural network, an impressive training accuracy of 0.99 was attained. However, its validation accuracy registered at 0.6, hinting at potential overfitting. An analysis of the test set predictions, as depicted by the confusion matrix, demonstrated that the correct classes were predominantly predicted for the corresponding sentences. Notably, the test set metrics were: precision at 0.595, recall at 0.569, and the f1 score also at 0.569.\nThe employment of support vector machines (SVM) with the bag of words approach, post-tuning, yielded training and validation accuracies of 0.989 and 0.533 respectively, with the test accuracy being 0.55. The precision, recall, and f1 scores for this model stood at 0.551, 0.55, and 0.547 respectively.\nLastly, when Naive Bayes was paired with the bag of words method, post-optimisation, it achieved training and validation accuracies of 0.89 and 0.592 respectively. This model appeared less prone to overfitting compared to its counterparts. For the test set, precision was measured at 0.624, recall at 0.615, and interestingly, the test accuracy was noted to be 0.616, slightly higher than the recall.\n\n\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the Bag of Words representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.994\n0.601\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.595\n\n\nRecall\n0.575\n\n\nF1 Score\n0.569\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.295\n0.294\n0.294\n\n\n0.010\n0.304\n0.309\n0.306\n\n\n0.100\n0.391\n0.322\n0.325\n\n\n0.500\n0.748\n0.425\n0.445\n\n\n1.000\n0.920\n0.527\n0.566\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n100.000\n0.995\n0.498\n0.517\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.551\n\n\nRecall\n0.55\n\n\nF1 Score\n0.547\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn accuracy plot for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.906\n0.572\n0.597\n\n\n0.010\n0.904\n0.581\n0.608\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n1.000\n0.838\n0.586\n0.611\n\n\n10.000\n0.637\n0.494\n0.523\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.624\n\n\nRecall\n0.615\n\n\nF1 Score\n0.616"
  },
  {
    "objectID": "results.html#bag-of-words",
    "href": "results.html#bag-of-words",
    "title": "Results",
    "section": "",
    "text": "In the exploration of various models on a training set employing a bag of words representation, distinct performance disparities were observed. Firstly, utilising a feed-forward neural network, an impressive training accuracy of 0.99 was attained. However, its validation accuracy registered at 0.6, hinting at potential overfitting. An analysis of the test set predictions, as depicted by the confusion matrix, demonstrated that the correct classes were predominantly predicted for the corresponding sentences. Notably, the test set metrics were: precision at 0.595, recall at 0.569, and the f1 score also at 0.569.\nThe employment of support vector machines (SVM) with the bag of words approach, post-tuning, yielded training and validation accuracies of 0.989 and 0.533 respectively, with the test accuracy being 0.55. The precision, recall, and f1 scores for this model stood at 0.551, 0.55, and 0.547 respectively.\nLastly, when Naive Bayes was paired with the bag of words method, post-optimisation, it achieved training and validation accuracies of 0.89 and 0.592 respectively. This model appeared less prone to overfitting compared to its counterparts. For the test set, precision was measured at 0.624, recall at 0.615, and interestingly, the test accuracy was noted to be 0.616, slightly higher than the recall.\n\n\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the Bag of Words representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.994\n0.601\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.595\n\n\nRecall\n0.575\n\n\nF1 Score\n0.569\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.295\n0.294\n0.294\n\n\n0.010\n0.304\n0.309\n0.306\n\n\n0.100\n0.391\n0.322\n0.325\n\n\n0.500\n0.748\n0.425\n0.445\n\n\n1.000\n0.920\n0.527\n0.566\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n100.000\n0.995\n0.498\n0.517\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.551\n\n\nRecall\n0.55\n\n\nF1 Score\n0.547\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn accuracy plot for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.906\n0.572\n0.597\n\n\n0.010\n0.904\n0.581\n0.608\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n1.000\n0.838\n0.586\n0.611\n\n\n10.000\n0.637\n0.494\n0.523\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.624\n\n\nRecall\n0.615\n\n\nF1 Score\n0.616"
  },
  {
    "objectID": "results.html#tf-idf",
    "href": "results.html#tf-idf",
    "title": "Results",
    "section": "TF-IDF",
    "text": "TF-IDF\nIn a subsequent analysis utilising the term frequency-inverse document frequency (tf-idf) representation with various models, certain resemblances to the bag of words (bow) results were discerned. Firstly, with the feed-forward neural network, the accuracy plot bore a striking similarity to its bow counterpart. This model achieved a training accuracy of 0.99 and a validation accuracy of 0.588. The confusion matrix for test set predictions indicated that the majority of sentences were assigned their correct classes. The test set metrics recorded were: precision at 0.598, recall at 0.597, and the f1 score at 0.595.\nWhen the support vector machines (SVM) were employed in tandem with the tf-idf representation, the accuracy plot was found to mirror that of the bow version. After tuning, the training accuracy registered at 0.968, with validation and test accuracies being 0.542 and 0.574, respectively. The precision, recall, and f1 scores for this model were 0.58, 0.574, and 0.573 in that order.\nLastly, the Naive Bayes model with the tf-idf approach displayed accuracy plots bearing a resemblance to the bow version. Post-optimisation, it yielded training, validation, and test accuracies of 0.915, 0.594, and 0.611 respectively. The precision and recall both stood at 0.611, whilst the test accuracy was slightly lower at 0.609.\n\nNeural network\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the TF-IDF representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.994\n0.588\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.598\n\n\nRecall\n0.597\n\n\nF1 Score\n0.595\n\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n1.000\n0.968\n0.542\n0.574\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.295\n0.294\n0.294\n\n\n0.010\n0.300\n0.296\n0.288\n\n\n0.100\n0.364\n0.306\n0.301\n\n\n0.500\n0.832\n0.435\n0.439\n\n\n1.000\n0.968\n0.542\n0.574\n\n\n10.000\n0.995\n0.531\n0.550\n\n\n100.000\n0.996\n0.528\n0.544\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the TF-IDF representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.58\n\n\nRecall\n0.574\n\n\nF1 Score\n0.573\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\nAn accuracy plot for the NB model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.100\n0.915\n0.594\n0.611\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.931\n0.575\n0.594\n\n\n0.010\n0.929\n0.584\n0.605\n\n\n0.100\n0.915\n0.594\n0.611\n\n\n1.000\n0.812\n0.575\n0.590\n\n\n10.000\n0.622\n0.500\n0.519\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the TF-IDF representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.611\n\n\nRecall\n0.611\n\n\nF1 Score\n0.609"
  },
  {
    "objectID": "results.html#token-embeddings",
    "href": "results.html#token-embeddings",
    "title": "Results",
    "section": "Token Embeddings",
    "text": "Token Embeddings\nUpon utilising text embedding as a representation technique alongside various models, a marked degradation in performance was observed compared to other preprocessing methods. With the feed-forward neural network, the training accuracy was a mere 0.409, while the validation accuracy dropped further to 0.369. The confusion matrix for test set predictions was quite telling: for a majority of sentences, the correct classes were not discerned. Intriguingly, the class “Zuma” was predominantly predicted. The test set showcased a precision of 0.367, recall of 0.368, and a notably lower f1 score of 0.328.\nWhen paired with the support vector machines (SVM), post-tuning, the training accuracy stood at 0.406, with validation and test accuracies of 0.361 and 0.347, respectively. The precision was 0.342, the recall was 0.347, and the f1 score was slightly lower at 0.319.\nIncorporating the Naive Bayes model with text embedding, post-optimisation, the training, validation, and test accuracies were 0.359, 0.359, and 0.338 in that order. This model’s precision and recall registered at 0.335 and 0.338 respectively, with the test accuracy being considerably reduced to 0.291. This underlines the challenge posed by text embeddings in this specific context, as the results were notably inferior to other data preparation methods.\n\nNeural network\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the text embeddings representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the text embeddings representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.409\n0.369\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the text embeddings representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the text embeddings representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.367\n\n\nRecall\n0.368\n\n\nF1 Score\n0.328\n\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.500\n0.406\n0.361\n0.347\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.257\n0.257\n0.257\n\n\n0.010\n0.300\n0.300\n0.292\n\n\n0.100\n0.368\n0.340\n0.335\n\n\n0.500\n0.406\n0.361\n0.347\n\n\n1.000\n0.451\n0.353\n0.341\n\n\n10.000\n0.574\n0.324\n0.338\n\n\n100.000\n0.687\n0.334\n0.327\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the text embedding representation, with the best hyperparameters highlighted below.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the text embedding representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.342\n\n\nRecall\n0.347\n\n\nF1 Score\n0.319\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\nAn accuracy plot for the NB model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.010\n0.359\n0.359\n0.338\n\n\n0.100\n0.359\n0.359\n0.338\n\n\n1.000\n0.359\n0.359\n0.338\n\n\n10.000\n0.359\n0.359\n0.338\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the text embedding representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the text embedding representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.335\n\n\nRecall\n0.338\n\n\nF1 Score\n0.291"
  },
  {
    "objectID": "results.html#bert-embeddings-with-pre-trained-classifier",
    "href": "results.html#bert-embeddings-with-pre-trained-classifier",
    "title": "Results",
    "section": "BERT Embeddings with pre-trained classifier",
    "text": "BERT Embeddings with pre-trained classifier\nUtilising the BERT embedding in tandem with a pre-trained model, a strategy known as transfer learning, distinctive patterns in performance were observed. Throughout the training epochs, the training accuracy showcased a consistent uptick. However, the validation accuracy plateaued rather swiftly, exhibiting minimal fluctuations thereafter. At the culmination of the training, the accuracy metrics stood as follows: training accuracy at 0.759, validation accuracy at 0.684, and a slightly higher test accuracy of 0.712. Further delving into the test set metrics, the precision was 0.71, recall was 0.707, and the f1 score was close behind at 0.708. An examination of the confusion matrix for the test set underscored these findings. The model predominantly made accurate predictions for the respective presidents, mirroring the positive metrics mentioned earlier. This highlights the efficacy of the BERT embeddings and transfer learning in this particular context, as the results were substantially more favourable than some other methods previously explored.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naccuracy\nprecision\nrecall\nf1_score\n\n\n\n\nTraining\n0.759\n0.507\n0.837\n0.503\n\n\nValidation\n0.684\n0.548\n0.744\n0.543\n\n\nTest\n0.712\n0.710\n0.707\n0.708"
  },
  {
    "objectID": "discussion_conclusion.html",
    "href": "discussion_conclusion.html",
    "title": "Discussion and Conclusion",
    "section": "",
    "text": "This study aimed to figure out which of the South African presidents, from 1994 to 2022, might have said certain sentences during their State of the Nation Address (SONA). Different ways of processing the text, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings, were used. These methods were then paired with machine learning models to see which combination worked best.\nWith the Bag of Words (BoW) method, the feed-forward neural network did well in training but not as well in validation, suggesting it might not do well with new, unseen data. The SVM and Naive Bayes models had similar outcomes. The tf-idf method gave results close to BoW for the neural net and SVM, but Naive Bayes seemed a bit more stable. However, simple text embeddings didn’t work as well across the board. This could be because these embeddings might be too basic to capture the unique way presidents speak in their SONA addresses.\nOn the other hand, using BERT embeddings with a pre-trained model gave us some hope. The model kept getting better during training, and its test results were the best among all the models. This suggests that using advanced methods like BERT might be the way forward for such tasks."
  },
  {
    "objectID": "discussion_conclusion.html#discussion",
    "href": "discussion_conclusion.html#discussion",
    "title": "Discussion and Conclusion",
    "section": "",
    "text": "This study aimed to figure out which of the South African presidents, from 1994 to 2022, might have said certain sentences during their State of the Nation Address (SONA). Different ways of processing the text, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings, were used. These methods were then paired with machine learning models to see which combination worked best.\nWith the Bag of Words (BoW) method, the feed-forward neural network did well in training but not as well in validation, suggesting it might not do well with new, unseen data. The SVM and Naive Bayes models had similar outcomes. The tf-idf method gave results close to BoW for the neural net and SVM, but Naive Bayes seemed a bit more stable. However, simple text embeddings didn’t work as well across the board. This could be because these embeddings might be too basic to capture the unique way presidents speak in their SONA addresses.\nOn the other hand, using BERT embeddings with a pre-trained model gave us some hope. The model kept getting better during training, and its test results were the best among all the models. This suggests that using advanced methods like BERT might be the way forward for such tasks."
  },
  {
    "objectID": "discussion_conclusion.html#conclusion",
    "href": "discussion_conclusion.html#conclusion",
    "title": "Discussion and Conclusion",
    "section": "Conclusion:",
    "text": "Conclusion:\nThis study shows how important it is to pick the right method to process text and the right model to analyse it. While methods like BoW and tf-idf gave decent results, simple text embeddings didn’t do as well. But, the combination of BERT embeddings and a pre-trained model stood out.\nThis has two main takeaways. First, for researchers looking into political speeches, these models can help in figuring out who might have said an unattributed speech. Second, for those into machine learning, it highlights the growing role of advanced methods like BERT.\nLooking ahead, it might be worth exploring even better text processing methods or fine-tuning models like BERT for even more accurate results. Overall, this study shows the exciting possibilities when combining tech with the study of political speeches."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The State of the Nation Address of the President of South Africa (SONA) is an annual event in which the President of South Africa reports on the status of the nation, normally to the resumption of a joint sitting of Parliament. You have been provided with full text of all State of the Nation Address (SONA) speeches, from 1994 through to 2022 . In years that elections took place, a State of the Nation Address happens twice, once before and again after the election\nThe dataset (sourced from https://www.gov.za/state-nation-address) was comprised of a series of text files of the State of the Nation Addresses (SONA) from 1994 through 2022. Each speech’s content was subsequently ingested, omitting the initial lines. These speeches were then collated into a structured format for more convenient access and manipulation.\nSubsequently, essential metadata, including the year of the address and the name of the delivering president, were gleaned from the file naming conventions. Ater that, the removal of URLs, HTML character codes, and newline characters was performed. Additionally, the date of each address was extracted and appropriately formatted.\nTo achieve the project’s objectives, each speech was dissected into its individual sentences. This granular breakdown facilitated the mapping of each sentence to its originating president. The finalised structured dataset comprises individual sentences paired with their respective presidents. This dataset was also saved as a csv file for future use.\n\n\n\n\n\n\n\nThe visualisation above illustrates the total number of speeches given by each president. Mbeki and Zuma had most speeches in the dataset, with 10 each. This means that there’s a substantial amount of data available for them, which could be advantageous when discerning their linguistic patterns, given that there is not a significant overlap in the sentences of the two presidents. Motlanthe and de Klerk only had one speech each, which may be an issue, due to an imbalance in the data, which may bias the model output later. To explore this further, the number of sentences per president is examined.\n\n\n\n\n\n\n\n\nThe chart above gives a breakdown of the number of sentences spoken by each president. Zuma stands out with the most sentences, further underscoring his prominence in the dataset. Notably, while Mbeki gave three more speeches than Ramaphosa, their sentence count is nearly the same, implying that Ramaphosa’s speeches might be more verbose or detailed. This data provides a deeper understanding of the granularity of each president’s contribution and reaffirms the potential data imbalance to be addressed in model development, especially when considering the fact that de Klerk and Motlanthe have less than 300 sentences each, while the others have well over 1500.\n\n\n\n\n\n\n\n\nThis chart unveils the average sentence length, in words, for each president. A striking observation is that Zuma, despite having the most sentences and speeches, has a relatively concise average sentence length. Conversely, Mbeki and Motlanthe have longer average sentence lengths, with Mbeki being the only president that had over 30 words per sentence, on average. This metric offers insights into the verbosity and style of each president, which can be a useful feature when discerning speech patterns in model building.\n\n\n\n\n\n\n\n\nThe word clouds offer a visually compelling representation of the most frequently used words by each president. The size of each word in the cloud corresponds to its frequency in the speeches. All the presidents had “will” as their most prominent word and referred to the country many times while speaking (highlighted by the use of the words “south” and “africa”/“african”). Motlanthe seemed to focus more on the economy and public image with the use of words such as “national”, “public” and “government”, whereas Mandela seemed to focus more on the people with the use of words such as “people” and “us”. de Klerk focused more on the constitution and forming alliances during a transitional period, and Zuma focused more on work and the development. These word clouds provide a snapshot of the focal points and themes of each president’s speeches. Distinctive words or terms can be potential features when building predictive models. The words from the wordclouds can also be seen in the bar plots below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of only looking at single word frequency, bigrams can also be used to find the most common two-word phrases. The bigrams above elucidate the distinctive linguistic patterns and thematic foci of each president, presenting opportunities for differentiation. For instance, President Mandela’s frequently used bigrams, such as “South Africans” and “national unity,” reflect his emphasis on nation-building and reconciliation during his tenure. In contrast, President Zuma’s bigrams like “economic growth” suggest a policy-driven discourse concentrated on economic dynamics. However, there are potential pitfalls. Overlapping or common bigrams across presidents, such as generic terms or phrases prevalent in political discourse, could introduce ambiguity, potentially hindering the model’s precision. Additionally, while President Ramaphosa’s bigrams like “South Africa” are distinctly frequent, they are not uniquely attributable to him, as such phrases are likely universal across South African presidencies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpanding on the analysis of linguistic markers, trigrams offer insights into the most recurrent three-word sequences employed by each president. The trigram outputs above further refine our understanding of the unique verbal choices and thematic concerns of each leader. For instance, President Mandela’s recurrent trigrams, such as “people of South Africa”, underscore his consistent focus on inclusive nationhood and the collective identity of South Africans. Meanwhile, President Zuma’s trigrams, such as “fellow South Africans,” indicate a direct and inclusive addressal style, perhaps aiming for rapport-building with the populace. Conversely, the presence of generic or universally applicable trigrams, such as “State of the Nation”, might pose challenges. These broadly-used trigrams, inherent to political addresses across presidencies, might dilute the distinctive features of individual presidents, complicating the model’s task. Moreover, trigrams like “State of the Nation Address” from President Ramaphosa, although salient, are emblematic of speeches common to all presidents, making them less distinguishing. Thus, while trigrams can accentuate the nuances of each president’s discourse, the model would benefit from discerning the balance between distinctiveness and generic trigram usage.\n\n\n\n\n\n\n\n\n\n\n\nThe Bag-of-Words (BoW) visualisation above reveals a pronounced central cluster with substantial overlap across presidential sentences, indicating pervasive shared linguistic elements. This convergence towards common terms suggests that the BoW representation predominantly captures universal themes and terminologies characteristic of political discourse. Such patterns, while illuminating shared linguistic tendencies, underscore potential challenges in predictive modeling, with the BoW approach possibly lacking the granularity to detect distinctive linguistic markers for each president.\n\n\n\n\n\n\n\n\nUsing the TF-IDF representation, the visualization depicts a dominant central cluster, reaffirming the presence of overlapping linguistic constructs across presidential discourses. Unlike the BoW representation, the TF-IDF visualization lacks discernible smaller clusters, and data points appear more dispersed. This dispersion underscores the varied thematic undertones each president might have explored, but the pronounced overlap in the central region suggests that these thematic variations are not sufficiently distinct in the TF-IDF space to provide clear demarcations. The observed patterns emphasize the challenges inherent in solely relying on TF-IDF for capturing the unique linguistic nuances of each president.\n\n\n\n\n\n\n\n\nUtilising tokenization with padding, the resultant visualization presents multiple clusters, indicating the method’s ability to recognize shared linguistic constructs or thematic groupings within the dataset. Notably, the significant intermingling of presidents within these clusters underscores the shared nature of discourse patterns across different presidencies. The absence of a dominant central cluster, a divergence from the BoW and TF-IDF representations, alludes to a more nuanced and diverse sentence representation in the embedding space, potentially attributed to the emphasis on sentence structure inherent in the tokenization method."
  },
  {
    "objectID": "eda.html#number-of-speeches-per-president",
    "href": "eda.html#number-of-speeches-per-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The visualisation above illustrates the total number of speeches given by each president. Mbeki and Zuma had most speeches in the dataset, with 10 each. This means that there’s a substantial amount of data available for them, which could be advantageous when discerning their linguistic patterns, given that there is not a significant overlap in the sentences of the two presidents. Motlanthe and de Klerk only had one speech each, which may be an issue, due to an imbalance in the data, which may bias the model output later. To explore this further, the number of sentences per president is examined."
  },
  {
    "objectID": "eda.html#number-of-sentences-per-president",
    "href": "eda.html#number-of-sentences-per-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The chart above gives a breakdown of the number of sentences spoken by each president. Zuma stands out with the most sentences, further underscoring his prominence in the dataset. Notably, while Mbeki gave three more speeches than Ramaphosa, their sentence count is nearly the same, implying that Ramaphosa’s speeches might be more verbose or detailed. This data provides a deeper understanding of the granularity of each president’s contribution and reaffirms the potential data imbalance to be addressed in model development, especially when considering the fact that de Klerk and Motlanthe have less than 300 sentences each, while the others have well over 1500."
  },
  {
    "objectID": "eda.html#average-sentence-length-per-president",
    "href": "eda.html#average-sentence-length-per-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This chart unveils the average sentence length, in words, for each president. A striking observation is that Zuma, despite having the most sentences and speeches, has a relatively concise average sentence length. Conversely, Mbeki and Motlanthe have longer average sentence lengths, with Mbeki being the only president that had over 30 words per sentence, on average. This metric offers insights into the verbosity and style of each president, which can be a useful feature when discerning speech patterns in model building."
  },
  {
    "objectID": "eda.html#word-clouds-for-each-president",
    "href": "eda.html#word-clouds-for-each-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The word clouds offer a visually compelling representation of the most frequently used words by each president. The size of each word in the cloud corresponds to its frequency in the speeches. All the presidents had “will” as their most prominent word and referred to the country many times while speaking (highlighted by the use of the words “south” and “africa”/“african”). Motlanthe seemed to focus more on the economy and public image with the use of words such as “national”, “public” and “government”, whereas Mandela seemed to focus more on the people with the use of words such as “people” and “us”. de Klerk focused more on the constitution and forming alliances during a transitional period, and Zuma focused more on work and the development. These word clouds provide a snapshot of the focal points and themes of each president’s speeches. Distinctive words or terms can be potential features when building predictive models. The words from the wordclouds can also be seen in the bar plots below."
  },
  {
    "objectID": "eda.html#n-gram-frequency-distributions-for-each-president",
    "href": "eda.html#n-gram-frequency-distributions-for-each-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Instead of only looking at single word frequency, bigrams can also be used to find the most common two-word phrases. The bigrams above elucidate the distinctive linguistic patterns and thematic foci of each president, presenting opportunities for differentiation. For instance, President Mandela’s frequently used bigrams, such as “South Africans” and “national unity,” reflect his emphasis on nation-building and reconciliation during his tenure. In contrast, President Zuma’s bigrams like “economic growth” suggest a policy-driven discourse concentrated on economic dynamics. However, there are potential pitfalls. Overlapping or common bigrams across presidents, such as generic terms or phrases prevalent in political discourse, could introduce ambiguity, potentially hindering the model’s precision. Additionally, while President Ramaphosa’s bigrams like “South Africa” are distinctly frequent, they are not uniquely attributable to him, as such phrases are likely universal across South African presidencies."
  },
  {
    "objectID": "eda.html#trigrams",
    "href": "eda.html#trigrams",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Expanding on the analysis of linguistic markers, trigrams offer insights into the most recurrent three-word sequences employed by each president. The trigram outputs above further refine our understanding of the unique verbal choices and thematic concerns of each leader. For instance, President Mandela’s recurrent trigrams, such as “people of South Africa”, underscore his consistent focus on inclusive nationhood and the collective identity of South Africans. Meanwhile, President Zuma’s trigrams, such as “fellow South Africans,” indicate a direct and inclusive addressal style, perhaps aiming for rapport-building with the populace. Conversely, the presence of generic or universally applicable trigrams, such as “State of the Nation”, might pose challenges. These broadly-used trigrams, inherent to political addresses across presidencies, might dilute the distinctive features of individual presidents, complicating the model’s task. Moreover, trigrams like “State of the Nation Address” from President Ramaphosa, although salient, are emblematic of speeches common to all presidents, making them less distinguishing. Thus, while trigrams can accentuate the nuances of each president’s discourse, the model would benefit from discerning the balance between distinctiveness and generic trigram usage."
  },
  {
    "objectID": "eda.html#bag-of-words-bow-representation",
    "href": "eda.html#bag-of-words-bow-representation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The Bag-of-Words (BoW) visualisation above reveals a pronounced central cluster with substantial overlap across presidential sentences, indicating pervasive shared linguistic elements. This convergence towards common terms suggests that the BoW representation predominantly captures universal themes and terminologies characteristic of political discourse. Such patterns, while illuminating shared linguistic tendencies, underscore potential challenges in predictive modeling, with the BoW approach possibly lacking the granularity to detect distinctive linguistic markers for each president."
  },
  {
    "objectID": "eda.html#tf-idf-representation",
    "href": "eda.html#tf-idf-representation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Using the TF-IDF representation, the visualization depicts a dominant central cluster, reaffirming the presence of overlapping linguistic constructs across presidential discourses. Unlike the BoW representation, the TF-IDF visualization lacks discernible smaller clusters, and data points appear more dispersed. This dispersion underscores the varied thematic undertones each president might have explored, but the pronounced overlap in the central region suggests that these thematic variations are not sufficiently distinct in the TF-IDF space to provide clear demarcations. The observed patterns emphasize the challenges inherent in solely relying on TF-IDF for capturing the unique linguistic nuances of each president."
  },
  {
    "objectID": "eda.html#tokenization-with-padding-representation",
    "href": "eda.html#tokenization-with-padding-representation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Utilising tokenization with padding, the resultant visualization presents multiple clusters, indicating the method’s ability to recognize shared linguistic constructs or thematic groupings within the dataset. Notably, the significant intermingling of presidents within these clusters underscores the shared nature of discourse patterns across different presidencies. The absence of a dominant central cluster, a divergence from the BoW and TF-IDF representations, alludes to a more nuanced and diverse sentence representation in the embedding space, potentially attributed to the emphasis on sentence structure inherent in the tokenization method."
  },
  {
    "objectID": "intro_lit_review.html",
    "href": "intro_lit_review.html",
    "title": "Introduction and Literature Review",
    "section": "",
    "text": "In the socio-political landscape, the manner and content of how leaders communicate provides critical insight into their governance style, priorities, and ideology. The State of the Nation Address (SONA) serves as an essential touchstone in South Africa’s political calendar, where the sitting president not only provides an annual report on the nation’s status but also sets the tone for policy directions and government intent for the subsequent year. Delivered at the commencement of a joint sitting of Parliament, particularly in election years, this address receives heightened scrutiny, given that it occurs twice: pre- and post-election.\nNatural Language Processing (NLP) has been increasingly leveraged in the domain of political science to uncover patterns, biases, and ideologies in the speeches and writings of political leaders. Recent advancements in machine learning and NLP tools have enabled more refined text analysis, going beyond mere word frequency to semantic content and stylistic nuances. Researchers such as Katre (2019) and Glavas, Nanni and Ponzetto (2019) have demonstrated the efficacy of using NLP to categorise and analyse political speeches. This raises the question: Can we discern, based purely on textual analysis, which South African president might have uttered a particular sentence during their SONA speech? In other words - can we predict the author of a sentence based on the content and style of the sentence?\nHowever, while there is an abundance of literature on NLP applications in sentiment analysis and topic modelling, its application to discern between specific authors or speakers, especially in the South African political sphere, remains largely unexplored. This gap is particularly noticeable when considering the unique linguistic, cultural, and political landscape of South Africa. The challenges lie not just in the variety of linguistic styles but also in the depth and breadth of topics covered, as well as the personal idiosyncrasies of each president (within a single speech as well as over time).\nGiven the above context, this paper aims to predict which of the South African presidents between 1994 and 2022 might have said a specific sentence during their SONA address. It leverages various text transformation techniques, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings (a very simple embedding as well as BERT). Subsequent application of machine learning models, including a feed-forward neural net, Support Vector Machine (SVM), Naive Bayes, and a BERT classification model, offers a comparative lens to evaluate the efficacy of each approach.\n\n\n\n\nReferences\n\nGlavaš, Goran, Federico Nanni, and Simone Paolo Ponzetto. 2019. “Computational Analysis of Political Texts: Bridging Research Efforts Across Communities.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, 18–23. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-4004.\n\n\nKatre, Paritosh. 2019. “NLP Based Text Analytics and Visualization of Political Speeches.” International Journal of Recent Technology and Engineering 8 (September): 8574–79. https://doi.org/10.35940/ijrte.C6503.098319."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]